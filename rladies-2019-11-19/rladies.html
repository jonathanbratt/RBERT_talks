<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>RBERT: Cutting Edge NLP in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jonathan Bratt &amp; Jon Harmon" />
    <meta name="date" content="2019-11-19" />
    <link href="rladies_files/remark-css/default.css" rel="stylesheet" />
    <link href="rladies_files/remark-css/hygge.css" rel="stylesheet" />
    <link href="rladies_files/remark-css/robot-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">


name: title
class: inverse, center, middle



&lt;img src="img/rbert_hex.png" width = "300px"/&gt;

## Cutting-Edge NLP in R

.Large[Jonathan Bratt | R-Ladies Austin | 2019-11-19]

---

name: acknowledgement

# RBERT Authors


.pull-left[
## Jonathan Bratt
###github.com/jonathanbratt
## Jon Harmon
###github.com/jonthegeek
##Macmillan Learning
]

.pull-right[

![Jon Harmon](img/harmon-action.png)]

---

name: outline

# Tonight

.Large[

* What's BERT?
* How can I use it in R?
]

---

# Deep Learning

.Large[
* Very powerful, but requires massive amounts of training data.
* *Transfer learning* can help.
]


---

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
]
]

---

count: false

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
* Early layers of the neural net: simple features
]
]

&lt;div width="50%"&gt;
&lt;p  style="padding-left:10"&gt;Credit: &lt;a href="http://yosinski.com/deepvis"&gt;deepvis&lt;/a&gt; by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson (2015)&lt;/p&gt;
&lt;div width="50%" style="float:left"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-dark_to_light.png" width="130px"\&gt;
&lt;/div&gt;
&lt;div width="50%" style="float:right"&gt;
&lt;/div&gt;
&lt;/div&gt;

---

count: false

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
* Early layers of the neural net: simple features
]
]

&lt;div width="50%"&gt;
&lt;p  style="padding-left:10"&gt;Credit: &lt;a href="http://yosinski.com/deepvis"&gt;deepvis&lt;/a&gt; by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson (2015)&lt;/p&gt;
&lt;div width="50%" style="float:left"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-dark_to_light.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-light_to_dark.png" width="130px"\&gt;
&lt;/div&gt;
&lt;div width="50%" style="float:right"&gt;
&lt;/div&gt;
&lt;/div&gt;

---

count: false

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
* Early layers of the neural net: simple features
* Later layers: complex features
]
]
 
&lt;div width="50%"&gt;
&lt;p  style="padding-left:10"&gt;Credit: &lt;a href="http://yosinski.com/deepvis"&gt;deepvis&lt;/a&gt; by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson (2015)&lt;/p&gt;
&lt;div width="50%" style="float:left"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-dark_to_light.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-light_to_dark.png" width="130px"\&gt;
&lt;/div&gt;
&lt;div width="50%" style="float:left; padding-left:20px"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-faces.png" width="130px"\&gt;
&lt;/div&gt;
&lt;/div&gt;

---

count: false

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
* Early layers of the neural net: simple features
* Later layers: complex features
]
]
 
&lt;div width="50%"&gt;
&lt;p  style="padding-left:10"&gt;Credit: &lt;a href="http://yosinski.com/deepvis"&gt;deepvis&lt;/a&gt; by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson (2015)&lt;/p&gt;
&lt;div width="50%" style="float:left"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-dark_to_light.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-light_to_dark.png" width="130px"\&gt;
&lt;/div&gt;
&lt;div width="50%" style="float:left; padding-left:20px"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-faces.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-text.png" width="130px"\&gt;
&lt;/div&gt;
&lt;/div&gt;

---

count: false

# Transfer Learning: Computer Vision

.pull-left[
.Large[
* Training task: Classify images (ImageNet: ~14M examples)
* Early layers of the neural net: simple features
* Later layers: complex features
* Features are broadly applicable: learned layers can be "transferred" to other models and other tasks.
]
]
 
&lt;div width="50%"&gt;
&lt;p  style="padding-left:10"&gt;Credit: &lt;a href="http://yosinski.com/deepvis"&gt;deepvis&lt;/a&gt; by Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson (2015)&lt;/p&gt;
&lt;div width="50%" style="float:left"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-dark_to_light.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-light_to_dark.png" width="130px"\&gt;
&lt;/div&gt;
&lt;div width="50%" style="float:left; padding-left:20px"&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-faces.png" width="130px"\&gt;
&lt;img src="img/deep_viz/deep_viz_toolbox-text.png" width="130px"\&gt;
&lt;/div&gt;
&lt;/div&gt;

---



# Transfer Learning: NLP

.Large[
* Prior to 2018: word embeddings
  * word2vec (Google, 2013)
  * GloVe (Stanford, 2014)
  * fastText (Facebook, 2015)
]

---

count: false

# Transfer Learning: NLP

.Large[
* Prior to 2018: word embeddings
  * word2vec (Google, 2013)
  * GloVe (Stanford, 2014)
  * fastText (Facebook, 2015)
* Words are points in a high-dimensional vector space.
  *  "king" − "man" ≅ "queen" − "woman"
  *  "eat" − "ate" ≅ "see" − "saw"
]

---

count: false

# Transfer Learning: NLP

.Large[
* Prior to 2018: word embeddings
  * word2vec (Google, 2013)
  * GloVe (Stanford, 2014)
  * fastText (Facebook, 2015)
* Words are points in a high-dimensional vector space.
  *  "king" − "man" ≅ "queen" − "woman"
  *  "eat" − "ate" ≅ "see" − "saw"
* Limitation: context-agnostic. Each word has *one* embedding vector.
  * "I saw a branch on the bank."  
  * "I saw a branch of the bank."
]

---

count: false

# Transfer Learning: NLP

.Large[
* Prior to 2018: word embeddings
  * word2vec (Google, 2013)
  * GloVe (Stanford, 2014)
  * fastText (Facebook, 2015)
* Words are points in a high-dimensional vector space.
  *  "king" − "man" ≅ "queen" − "woman"
  *  "eat" − "ate" ≅ "see" − "saw"
* Limitation: context-agnostic. Each word has *one* embedding vector.
  * "I saw a *branch* **on** the *bank*."  
  * "I saw a *branch* **of** the *bank*."
]
&lt;img src="img/branch-on-bank.png" width = "220px"/&gt;
&lt;img src="img/branch-of-bank.png" width = "220px"/&gt;


---

# BERT

.Large[
* **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
* Released October 11, 2018 by Google AI Language team
* Training tasks: masked word prediction, sentence pair classification
* **Structure: multiple layers of "self-attention"**
* **Output: context-dependent embedding vectors**
]


---

# Package: RBERT

.pull-left[
.Large[
* `install_github("jonathanbratt/RBERT")`
* Implementation of BERT in R
* Use for:
  * Running pre-trained BERT models
  * Feature extraction (input text, output embeddings)
  * Soon: fine-tuning
]
]

.pull-right[
&lt;img src="img/rbert_hex.png" width="400px"&gt;
]

---

# Package: RBERTviz

.pull-left[
.Large[
* `install_github("jonathanbratt/RBERTviz")`
* For visualizing output from RBERT
  * `visualize_attention`
  * `display_pca`
]
]

.pull-right[
&lt;img src="img/RBERTviz.png" width="400px"&gt;
]

---

# Attention

.pull-left[

```r
RBERT::download_BERT_checkpoint(
  "bert_base_uncased"
)
RBERT::extract_features(
  "I love tacos.",
  model = "bert_base_uncased",
  layer_indexes = 1:12,
  features = "attention"
)$attention %&gt;%
  RBERTviz::visualize_attention()
```

Based on Jesse Vig's [bertviz](https://github.com/jessevig/bertviz) tool.
]
.pull-right[
&lt;img src="img/attention/tacos.png", width="500px"/&gt;
]

[Live demo](tacos_viz.html)

---

# Attention

.Large[
Sentences:

* The chicken didn't cross the road because it was too tired.
* The chicken didn't cross the road because it was too wide.
* The dog fetched the ball. It was excited.
* The dog fetched the ball. It was blue.
]

---

count: false

# Attention

.Large[
Sentences:

* The **chicken** didn't cross the road because **it** was too **tired.**
* The chicken didn't cross the **road** because **it** was too **wide.**
* The **dog** fetched the ball. **It** was **excited.**
* The dog fetched the **ball.** **It** was **blue.**
]

---

# Attention

&lt;img src="img/attention/3_1-chicken_tired.png" width="250px"&gt;
&lt;img src="img/attention/3_1-chicken_wide.png" width="250px"&gt;
&lt;img src="img/attention/3_1-dog_excited.png" width="250px"&gt;
&lt;img src="img/attention/3_1-dog_blue.png" width="250px"&gt;

.Large[Early layers: simple features (e.g. next word)]

---

# Attention

.pull-left[
&lt;img src="img/attention/9_5-chicken_tired.png"&gt;
]
.pull-right[
]


.Large[Later layers: sophisticated features (e.g. pronoun resolution)]

---

count: false

# Attention

.pull-left[
&lt;img src="img/attention/9_5-chicken_tired.png"&gt;
]
.pull-right[
&lt;img src="img/attention/9_5-chicken_wide.png"&gt;
]


.Large[Later layers: sophisticated features (e.g. pronoun resolution)]

---

count: false

# Attention

.pull-left[
&lt;img src="img/attention/9_5-dog_excited.png"&gt;
]
.pull-right[
&lt;img src="img/attention/9_5-dog_blue.png"&gt;
]


.Large[Later layers: sophisticated features (e.g. pronoun resolution)]



---

# Token Embeddings

.pull-left[
.Large[
From online survey, collected about 100 "train" sentences:

* "I could train a motivated gorilla."
* "I took the train to Chicago over Thanksgiving."
* "I wish I could be trained in how to use a jackhammer."
* "Trains are too noisy."
]]

.pull-right[

```r
trains_data &lt;- readRDS("trains_data.rds") %&gt;%
  dplyr::mutate(
    sequence_index = dplyr::row_number()
  )

trains_output &lt;- RBERT::extract_features(
  trains_data$sentence,
  model = "bert_base_uncased",
  layer_indexes = 0:12,
  features = "output"
)$output

trains_output_labeled &lt;- trains_output %&gt;%
  dplyr::left_join(
    dplyr::select(
      trains_data, 
      sequence_index, label
    ),
    by = "sequence_index"
  )
```
]



---

# Token Embeddings

.pull-left[


```r
trains_output_labeled %&gt;% 
  RBERTviz::display_pca(
    token_filter = "^train",
    layer_index = 0,
    # Just show one example of each unique word
    first_token_instance = TRUE
  )
```
]

.pull-right[
&lt;img src="rladies_files/figure-html/layer-0-plot-1.png" width="504" /&gt;
]

---

# Token Embeddings

Layer 0 (initial vectors)

&lt;img src="rladies_files/figure-html/layer-0-labeled-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 1

&lt;img src="rladies_files/figure-html/layer-1-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 2

&lt;img src="rladies_files/figure-html/layer-2-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 3

&lt;img src="rladies_files/figure-html/layer-3-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 4

&lt;img src="rladies_files/figure-html/layer-4-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 5

&lt;img src="rladies_files/figure-html/layer-5-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 6

&lt;img src="rladies_files/figure-html/layer-6-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 7

&lt;img src="rladies_files/figure-html/layer-7-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 8

&lt;img src="rladies_files/figure-html/layer-8-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 9

&lt;img src="rladies_files/figure-html/layer-9-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 10

&lt;img src="rladies_files/figure-html/layer-10-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

Layer 11

&lt;img src="rladies_files/figure-html/layer-11-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

wait a minute...

&lt;img src="rladies_files/figure-html/highlight-bad-1.png" width="1080" /&gt;
---
count: false

# Token Embeddings

wait a minute...

&lt;img src="rladies_files/figure-html/highlight-bad2-1.png" width="1080" /&gt;
---

# To Do

.Large[
* RBERT is usable *now*...
]

---
count: false

# To Do

.Large[
* RBERT is usable *now*...
* ...but it can be *better!*
]

---
count: false

# To Do

.Large[
* RBERT is usable *now*...
* ...but it can be *better!*
* Goal: CRAN by end of 2019
]

---
count: false

# To Do

.Large[
* RBERT is usable *now*...
* ...but it can be *better!*
* Goal: CRAN by end of 2019
  * TensorFlow 2.0 
  * More Rtful, less pythonic
  * Recipe: `step_bert_features()`
]

---
count: false

# To Do

.Large[
* RBERT is usable *now*...
* ...but it can be *better!*
* Goal: CRAN by end of 2019
  * TensorFlow 2.0 
  * More Rtful, less pythonic
  * Recipe: `step_bert_features()`
* `rstudio::conf(2020L)` e-poster
]

---

# Contact

.Large[
* [github.com/jonathanbratt/RBERT](github.com/jonathanbratt/RBERT)
* [github.com/jonathanbratt/RBERTviz](github.com/jonathanbratt/RBERTviz)
* [github.com/jonthegeek](github.com/jonthegeek)
* Twitter: [@jonthegeek](https://twitter.com/JonTheGeek)
* R4DS Online Learning Community: [r4ds.online](r4ds.online)
* TidyTuesday Podcast: [tidytuesday.com](tidytuesday.com)

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
